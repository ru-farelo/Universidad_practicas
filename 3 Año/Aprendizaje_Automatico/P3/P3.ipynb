{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Si lo que queremos conseguir es una RNA que de un error en test lo menor posible, ¿por qué no se escoge la RNA correspondiente al ciclo con un menor error en test?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Se escoje el mejor error de validación porque este a diferencia del de test tiene en cuenta el sobreajuste y la generalizacion de la RNA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Desarrollar una función llamada holdOut que dados dos parámetros, N (igual al número de patrones) y P (valor entre 0 y 1, que indica el porcentaje de patrones que se separarán para el conjunto de test), devuelva una tupla con dos vectores con los índices de los patrones que serán utilizados para entrenamiento y test. La suma de longitudes de ambos vectores tiene que ser igual a N, y estos dos vectores tienen que ser disjuntos. \n",
    "\n",
    "###### * Esta función se puede hacer en de una forma muy sencilla usando la función randperm (para usarla, cargar el módulo con using Random) \n",
    "\n",
    "###### A partir de esta función, partir una base de datos en dos subconjuntos se realiza de una forma inmediata. \n",
    "######  Para realizar esta función no se permite el uso de bucles. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "holdOut (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function holdOut(N::Int, P::Real)\n",
    "    if P < 0 || P > 1\n",
    "        error(\"P debe ser un valor entre 0 y 1\")\n",
    "    end\n",
    "    if N < 1\n",
    "        error(\"N debe ser un valor mayor a 0\")\n",
    "    end\n",
    "    indices = randperm(N)\n",
    "    n_test = round(Int, N*P)\n",
    "    return indices[n_test+1:end], indices[1:n_test]\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Desarrollar otra función llamada holdOut, igual que en la anterior y que, basándose en ella, tome 3 parámetros: N (número de patrones), Pval (tasa de patrones en el conjunto de validación) y Ptest (tasa de patrones en el conjunto de test), y devuelva una tupla con 3 vectores, con los índices de los elementos de los conjuntos de entrenamiento, validación y test. La suma de las longitudes de estos 3 vectores tiene que ser igual a N. \n",
    "###### * Para realizar esto, simplemente hay que hacer dos llamadas a la función holdOut desarrollada previamente. \n",
    "###### Al igual que en la función anterior, para realizar esta no se permite el uso de bucles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "holdOut (generic function with 2 methods)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function holdOut(N::Int, Pval::Real, Ptest::Real) \n",
    "    if Pval < 0 || Pval > 1 || Ptest < 0 || Ptest > 1\n",
    "        error(\"Pval y Ptest deben ser valores entre 0 y 1\")\n",
    "    end\n",
    "    if N < 1\n",
    "        error(\"N debe ser un valor mayor a 0\")\n",
    "    end\n",
    "    if Pval + Ptest > 1\n",
    "        error(\"La suma de Pval y Ptest debe ser menor o igual a 1\")\n",
    "    end\n",
    "    indices = randperm(N)\n",
    "    n_val = round(Int, N*Pval)\n",
    "    n_test = round(Int, N*Ptest)\n",
    "    return indices[n_val+n_test+1:end], indices[n_val+1:n_val+n_test], indices[1:n_val]\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ##### Modificar la función trainClassANN de entrenamiento de la RNA realizada en la práctica anterior para que acepte los siguientes parámetros opcionales a mayores de los definidos en la práctica anterior: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "trainClassANN (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function trainClassANN(topology::AbstractArray{<:Int,1}, \n",
    "    trainingDataset::Tuple{AbstractArray{<:Real,2}, AbstractArray{Bool,2}};\n",
    "    validationDataset::Tuple{AbstractArray{<:Real,2}, AbstractArray{Bool,2}}=\n",
    "        (Array{eltype(trainingDataset[1]),2}(undef,0,0), falses(0,0)),\n",
    "    testDataset::Tuple{AbstractArray{<:Real,2}, AbstractArray{Bool,2}}=\n",
    "        (Array{eltype(trainingDataset[1]),2}(undef,0,0), falses(0,0)),\n",
    "    transferFunctions::AbstractArray{<:Function,1}=fill(σ, length(topology)),\n",
    "    maxEpochs::Int=1000, minLoss::Real=0.0, learningRate::Real=0.01, \n",
    "    maxEpochsVal::Int=20)\n",
    "\n",
    "inputs = trainingDataset[1]'\n",
    "targets = trainingDataset[2]'\n",
    "numInputs = size(inputs, 1)\n",
    "numOutputs = size(targets, 1)\n",
    "ann = buildClassANN(numInputs, topology, numOutputs; transferFunctions=transferFunctions)\n",
    "\n",
    "loss(model, x, y) = (size(y,1) == 1) ? Losses.binarycrossentropy(model(x),y) : Losses.crossentropy(model(x),y)\n",
    "\n",
    "opt_state = Flux.setup(Adam(learningRate), ann)\n",
    "\n",
    "loss_vector_train = zeros(maxEpochs)\n",
    "loss_vector_val = zeros(maxEpochs)\n",
    "loss_vector_test = zeros(maxEpochs)\n",
    "\n",
    "best_ann = deepcopy(ann)\n",
    "best_loss_val = Inf\n",
    "patience = 0\n",
    "\n",
    "for epoch in 1:maxEpochs\n",
    "Flux.train!(loss, ann, [(inputs, targets)], opt_state)\n",
    "\n",
    "loss_vector_train[epoch] = loss(ann, inputs, targets)\n",
    "\n",
    "if !isempty(validationDataset[1])\n",
    "inputs_val = validationDataset[1]'\n",
    "targets_val = validationDataset[2]'\n",
    "loss_vector_val[epoch] = loss(ann, inputs_val, targets_val)\n",
    "\n",
    "if loss_vector_val[epoch] < best_loss_val\n",
    "best_loss_val = loss_vector_val[epoch]\n",
    "best_ann = deepcopy(ann)\n",
    "patience = 0\n",
    "else\n",
    "patience += 1\n",
    "if patience >= maxEpochsVal\n",
    " break\n",
    "end\n",
    "end\n",
    "end\n",
    "\n",
    "if !isempty(testDataset[1])\n",
    "inputs_test = testDataset[1]'\n",
    "targets_test = testDataset[2]'\n",
    "loss_vector_test[epoch] = loss(ann, inputs_test, targets_test)\n",
    "end\n",
    "end \n",
    "\n",
    "if isempty(validationDataset[1])\n",
    "best_ann = deepcopy(ann)\n",
    "end\n",
    "\n",
    "return (best_ann, loss_vector_train[1:epoch], loss_vector_val[1:epoch], loss_vector_test[1:epoch])\n",
    "end \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Dado que había dos versiones de la función de entrenamiento de RR.NN.AA. (la segunda aceptaba como salidas deseadas un vector), modificar de la misma manera esta función para que las salidas deseadas sean vectores en lugar de matrices para el caso de clasificación en dos clases. Tened en cuenta que en este caso los tipos de los conjuntos de entrenamiento, validación y test pasados como parámetros serán Tuple{AbstractArray{<:Real,2}, AbstractArray{Bool,1}} para los tres y por tanto será necesario llamar a la función reshape en los tres. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "trainClassANN (generic function with 2 methods)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function trainClassANN(topology::AbstractArray{<:Int,1},\n",
    "    (inputs, targets)::Tuple{AbstractArray{<:Real,2}, AbstractArray{Bool,1}};\n",
    "    validationDataset::Tuple{AbstractArray{<:Real,2}, AbstractArray{Bool,1}}=\n",
    "        (Array{eltype(inputs),2}(undef,0,0), falses(0)),\n",
    "    testDataset::Tuple{AbstractArray{<:Real,2}, AbstractArray{Bool,1}}=\n",
    "        (Array{eltype(inputs),2}(undef,0,0), falses(0)),\n",
    "    transferFunctions::AbstractArray{<:Function,1}=fill(σ, length(topology)),\n",
    "    maxEpochs::Int=1000, minLoss::Real=0.0, learningRate::Real=0.01, \n",
    "    maxEpochsVal::Int=20) \n",
    "\n",
    "targets = reshape(targets, (size(targets, 2), 1))  # Redimensionar a vector\n",
    "trainClassANN(topology, (inputs, targets); transferFunctions = transferFunctions, \n",
    "maxEpochs, minLoss, learningRate, maxEpochsVal)\n",
    "end\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.10.0",
   "language": "julia",
   "name": "julia-1.10"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
